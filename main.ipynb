{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb (or main.py)\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd # For logging results\n",
    "\n",
    "from utils.config_loader import load_config\n",
    "from data.celeba_loader import get_celeba_dataloaders\n",
    "from data.cub_loader import get_cub_dataloaders\n",
    "from data.imagenet_loader import get_imagenet_dataloaders\n",
    "from models.clip_wrapper import CLIPModelWrapper\n",
    "from surgery.direction_finder import compute_direction\n",
    "from surgery.edit_embedding import apply_surgery\n",
    "from baselines.prompt_tuning import prompt_tuning_baseline\n",
    "from baselines.random_edit import random_edit_baseline\n",
    "from evaluation.metrics import calculate_accuracy, calculate_cosine_similarity, calculate_mean_confidence\n",
    "from evaluation.visualize import create_tsne_plot, plot_cosine_similarity_hist\n",
    "\n",
    "def main():\n",
    "    # Load configuration\n",
    "    cfg = load_config(\"config.yaml\")\n",
    "    print(\"Configuration loaded:\")\n",
    "    for key, value in cfg.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(cfg[\"output_dir\"], exist_ok=True)\n",
    "    \n",
    "    # Phase 1: Data Preparation & Loading\n",
    "    train_loader, test_loader = None, None\n",
    "    if cfg['dataset'] == 'CelebA':\n",
    "        train_loader, test_loader = get_celeba_dataloaders(cfg)\n",
    "    elif cfg['dataset'] == 'CUB-200':\n",
    "        train_loader, test_loader = get_cub_dataloaders(cfg)\n",
    "    elif cfg['dataset'] == 'ImageNet':\n",
    "        train_loader, test_loader = get_imagenet_dataloaders(cfg)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset specified in config: {cfg['dataset']}\")\n",
    "\n",
    "    print(f\"\\nPhase 1: Data Preparation & Loading Complete.\")\n",
    "    print(f\"Train DataLoader batches: {len(train_loader)}\")\n",
    "    print(f\"Test DataLoader batches: {len(test_loader)}\")\n",
    "\n",
    "    # Phase 2: Model Setup & Latent Surgery Implementation\n",
    "    print(\"\\n--- Phase 2: Model Setup & Latent Surgery Implementation ---\")\n",
    "\n",
    "    clip = CLIPModelWrapper(cfg[\"model_name\"])\n",
    "\n",
    "    # Compute semantic direction vector using the training data\n",
    "    direction = compute_direction(clip, train_loader, cfg)\n",
    "    print(f\"Computed semantic direction vector shape: {direction.shape}\")\n",
    "\n",
    "    # Apply latent surgery to test embeddings\n",
    "    # We will get original and edited embeddings for the test set\n",
    "    edited_embeddings_surgery, original_embeddings_test, test_labels, test_img_ids = apply_surgery(\n",
    "        clip, test_loader, direction, cfg[\"surgery_alpha\"]\n",
    "    )\n",
    "    print(f\"Original test embeddings shape: {original_embeddings_test.shape}\")\n",
    "    print(f\"Edited test embeddings (Latent Surgery) shape: {edited_embeddings_surgery.shape}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}\")\n",
    "    print(f\"Number of test image IDs: {len(test_img_ids)}\")\n",
    "\n",
    "    print(\"\\nPhase 2: Model Setup & Latent Surgery Implementation Complete.\")\n",
    "\n",
    "    # Phase 3: Baseline Comparison & Evaluation\n",
    "    print(\"\\n--- Phase 3: Baseline Comparison & Evaluation ---\")\n",
    "\n",
    "    results = [] # To store all evaluation results\n",
    "\n",
    "    # --- Evaluation for Latent Surgery ---\n",
    "    print(\"\\nEvaluating Latent Surgery:\")\n",
    "    # For classification with latent surgery, we can use the direction vector itself as a classifier\n",
    "    # A simple classifier: dot product with direction vector, then threshold.\n",
    "    # More robust: use the concept text embeddings (from prompt tuning) as targets for both original and edited\n",
    "    \n",
    "    # Re-use prompt text embeddings as \"classifiers\"\n",
    "    pos_text_prompt = f\"A photo of a {cfg['concept']['positive'].lower()}\"\n",
    "    neg_text_prompt = f\"A photo of a {cfg['concept']['negative'].lower()}\"\n",
    "    text_embeddings = clip.embed_text([pos_text_prompt, neg_text_prompt]).to(original_embeddings_test.device)\n",
    "    pos_concept_emb = text_embeddings[0]\n",
    "    neg_concept_emb = text_embeddings[1]\n",
    "\n",
    "    # Evaluate original embeddings w.r.t. concept prompts\n",
    "    original_preds, original_confidences = prompt_tuning_baseline(\n",
    "        clip, original_embeddings_test, test_labels, cfg['concept']['positive'], cfg['concept']['negative']\n",
    "    ) # Re-using the prompt_tuning_baseline logic for evaluation\n",
    "    original_accuracy = calculate_accuracy(original_preds, test_labels)\n",
    "    original_mean_pos_conf, original_mean_neg_conf = calculate_mean_confidence(original_confidences, test_labels)\n",
    "    results.append({\n",
    "        'Method': 'Original Embeddings',\n",
    "        'Accuracy': original_accuracy,\n",
    "        'Mean_Positive_Confidence': original_mean_pos_conf,\n",
    "        'Mean_Negative_Confidence': original_mean_neg_conf,\n",
    "        'Cosine_Similarity_to_Original': 1.0 # By definition\n",
    "    })\n",
    "\n",
    "    # Evaluate edited embeddings w.r.t. concept prompts\n",
    "    edited_preds_surgery, edited_confidences_surgery = prompt_tuning_baseline(\n",
    "        clip, edited_embeddings_surgery, test_labels, cfg['concept']['positive'], cfg['concept']['negative']\n",
    "    )\n",
    "    surgery_accuracy = calculate_accuracy(edited_preds_surgery, test_labels)\n",
    "    surgery_mean_pos_conf, surgery_mean_neg_conf = calculate_mean_confidence(edited_confidences_surgery, test_labels)\n",
    "    \n",
    "    # Cosine similarity between original and edited embeddings (for change magnitude)\n",
    "    cos_sim_orig_to_surgery = calculate_cosine_similarity(original_embeddings_test, edited_embeddings_surgery).mean().item()\n",
    "\n",
    "    results.append({\n",
    "        'Method': 'Latent Surgery',\n",
    "        'Accuracy': surgery_accuracy,\n",
    "        'Mean_Positive_Confidence': surgery_mean_pos_conf,\n",
    "        'Mean_Negative_Confidence': surgery_mean_neg_conf,\n",
    "        'Cosine_Similarity_to_Original': cos_sim_orig_to_surgery\n",
    "    })\n",
    "\n",
    "    # --- Baselines ---\n",
    "    # Random Edit Baseline\n",
    "    if cfg.get('run_random_edit_baseline', True):\n",
    "        print(\"\\nEvaluating Random Edit Baseline:\")\n",
    "        edited_embeddings_random = random_edit_baseline(original_embeddings_test, alpha=cfg[\"surgery_alpha\"]) # Use same alpha\n",
    "        \n",
    "        random_preds, random_confidences = prompt_tuning_baseline(\n",
    "            clip, edited_embeddings_random, test_labels, cfg['concept']['positive'], cfg['concept']['negative']\n",
    "        )\n",
    "        random_accuracy = calculate_accuracy(random_preds, test_labels)\n",
    "        random_mean_pos_conf, random_mean_neg_conf = calculate_mean_confidence(random_confidences, test_labels)\n",
    "        cos_sim_orig_to_random = calculate_cosine_similarity(original_embeddings_test, edited_embeddings_random).mean().item()\n",
    "\n",
    "        results.append({\n",
    "            'Method': 'Random Edit Baseline',\n",
    "            'Accuracy': random_accuracy,\n",
    "            'Mean_Positive_Confidence': random_mean_pos_conf,\n",
    "            'Mean_Negative_Confidence': random_mean_neg_conf,\n",
    "            'Cosine_Similarity_to_Original': cos_sim_orig_to_random\n",
    "        })\n",
    "\n",
    "    # Prompt Tuning Baseline (already effectively run as the classification method)\n",
    "    # The 'Original Embeddings' entry in `results` already covers the performance of prompt tuning\n",
    "    # when applied directly to original embeddings. We don't need a separate \"run\" for this baseline.\n",
    "    # The prompt_tuning_baseline function itself is used as the classification mechanism.\n",
    "    \n",
    "    # --- Visualizations ---\n",
    "    if cfg.get('save_tsne', True):\n",
    "        # t-SNE plot for Original Embeddings\n",
    "        create_tsne_plot(\n",
    "            original_embeddings_test,\n",
    "            test_labels,\n",
    "            title=f\"t-SNE of Original Embeddings ({cfg['dataset']} - {cfg['concept']['positive']}/{cfg['concept']['negative']})\",\n",
    "            save_path=os.path.join(cfg['output_dir'], 'tsne_original.png')\n",
    "        )\n",
    "\n",
    "        # t-SNE plot for Latent Surgery Edited Embeddings\n",
    "        create_tsne_plot(\n",
    "            edited_embeddings_surgery,\n",
    "            test_labels,\n",
    "            title=f\"t-SNE of Latent Surgery Embeddings (Alpha={cfg['surgery_alpha']})\",\n",
    "            save_path=os.path.join(cfg['output_dir'], 'tsne_latent_surgery.png')\n",
    "        )\n",
    "\n",
    "        if cfg.get('run_random_edit_baseline', True):\n",
    "            # t-SNE plot for Random Edit Baseline Embeddings\n",
    "            create_tsne_plot(\n",
    "                edited_embeddings_random,\n",
    "                test_labels,\n",
    "                title=f\"t-SNE of Random Edit Embeddings (Alpha={cfg['surgery_alpha']})\",\n",
    "                save_path=os.path.join(cfg['output_dir'], 'tsne_random_edit.png')\n",
    "            )\n",
    "            \n",
    "        # Cosine similarity histogram of change from original to edited (Latent Surgery)\n",
    "        cos_sim_dist_surgery = calculate_cosine_similarity(original_embeddings_test, edited_embeddings_surgery)\n",
    "        plot_cosine_similarity_hist(\n",
    "            cos_sim_dist_surgery,\n",
    "            title=f\"Cosine Similarity (Original vs Latent Surgery Edited) Distribution\",\n",
    "            save_path=os.path.join(cfg['output_dir'], 'cosine_similarity_surgery_hist.png')\n",
    "        )\n",
    "\n",
    "        if cfg.get('run_random_edit_baseline', True):\n",
    "            # Cosine similarity histogram of change from original to edited (Random Edit)\n",
    "            cos_sim_dist_random = calculate_cosine_similarity(original_embeddings_test, edited_embeddings_random)\n",
    "            plot_cosine_similarity_hist(\n",
    "                cos_sim_dist_random,\n",
    "                title=f\"Cosine Similarity (Original vs Random Edit Edited) Distribution\",\n",
    "                save_path=os.path.join(cfg['output_dir'], 'cosine_similarity_random_hist.png')\n",
    "            )\n",
    "\n",
    "\n",
    "    # Log results to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv_path = os.path.join(cfg['output_dir'], 'evaluation_results.csv')\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"\\nEvaluation results saved to {results_csv_path}\")\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    print(\"\\nPhase 3: Baseline Comparison & Evaluation Complete.\")\n",
    "\n",
    "    # In Phase 4, we will focus on reporting, reproducibility, and documentation.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ensure config.yaml and dummy data setup from Phase 2 is in place\n",
    "    if not os.path.exists(\"config.yaml\"):\n",
    "        print(\"Please ensure config.yaml and dummy data setup from Phase 2 are in place or replace with real data paths.\")\n",
    "        print(\"Run the 'main()' function or directly create the necessary files/folders.\")\n",
    "    else:\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
